{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d757823",
   "metadata": {},
   "source": [
    "# Laboratorio 1 (AlpesHearth): Exploración, preparación y regresión lineal\n",
    "\n",
    "Este notebook está diseñado para **resolver el enunciado completo** del Laboratorio 1, siguiendo el estilo y las ideas de:\n",
    "- `ManipulacionExploracionCalidad.ipynb` (exploración + calidad: completitud, unicidad, consistencia, validez)\n",
    "- `LimpiezaTransformacion.ipynb` (limpieza + transformación)\n",
    "- `RegresionLineal.ipynb` (pipeline, métricas, y verificación de supuestos)\n",
    "\n",
    "**Objetivo técnico del laboratorio:** predecir `CVD Risk Score` (variable continua) con modelos de regresión lineal, comparar al menos 2 estrategias de preparación, interpretar coeficientes del mejor modelo y generar predicciones para el set de test no etiquetado.\n",
    "\n",
    "Nota de compatibilidad: `Datos Lab 1.csv` usa separador `,` y `Datos Test Lab 1.csv` usa separador `;`. Este notebook lo maneja automáticamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266465c6",
   "metadata": {},
   "source": [
    "## 0. Imports y constantes\n",
    "\n",
    "Qué haces aquí: importar librerías y fijar constantes exigidas por el enunciado.\n",
    "\n",
    "Por qué existe esta celda: evita imports repetidos y hace explícitos los parámetros obligatorios: `random_state=42` y `test_size=0.25`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f946fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "TARGET = \"CVD Risk Score\"\n",
    "AUX_LABEL = \"CVD Risk Level\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ba888",
   "metadata": {},
   "source": [
    "## 1. Cargar datos (train, test) y diccionario\n",
    "\n",
    "Qué haces aquí:\n",
    "1. Leer los CSV de train y test detectando separador automáticamente.\n",
    "2. Leer el diccionario en Excel para entender el significado de las columnas.\n",
    "\n",
    "Por qué existe esta celda:\n",
    "- El enunciado exige revisar el diccionario primero.\n",
    "- Si lees el test con separador incorrecto, todo el notebook falla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_csv_auto_sep(path: str) -> pd.DataFrame:\n",
    "    # Detecta si el separador es ; o , leyendo la primera línea del archivo\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        first_line = f.readline()\n",
    "    sep = \";\" if first_line.count(\";\") > first_line.count(\",\") else \",\"\n",
    "    return pd.read_csv(path, sep=sep)\n",
    "\n",
    "train_path = \"Datos Lab 1.csv\"\n",
    "test_path  = \"Datos Test Lab 1.csv\"\n",
    "dicc_path  = \"DiccPacientes.xlsx\"\n",
    "\n",
    "train_raw = read_csv_auto_sep(train_path)\n",
    "test_raw  = read_csv_auto_sep(test_path)\n",
    "dicc_raw  = pd.read_excel(dicc_path)\n",
    "\n",
    "# El diccionario trae el nombre de columna con un espacio final, lo normalizamos.\n",
    "dicc = dicc_raw.copy()\n",
    "dicc.columns = [c.strip() for c in dicc.columns]\n",
    "\n",
    "print(\"Train shape:\", train_raw.shape)\n",
    "print(\"Test  shape:\", test_raw.shape)\n",
    "display(dicc.head(10))\n",
    "\n",
    "# Validación rápida: train debe tener target, test no.\n",
    "print(\"Columnas extra en train (vs test):\", sorted(list(set(train_raw.columns) - set(test_raw.columns))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f386a3",
   "metadata": {},
   "source": [
    "## 2. Exploración inicial (descripción básica)\n",
    "\n",
    "Qué haces aquí:\n",
    "- Mirar primeras filas (`head`), tipos (`dtypes`) y resumen estadístico (`describe`).\n",
    "\n",
    "Por qué existe esta celda:\n",
    "- Es el punto de partida para detectar problemas de calidad y decidir preparación.\n",
    "- Esto sigue el enfoque de \"Descripción de los datos\" en los notebooks guía.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7933790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(train_raw.head())\n",
    "display(test_raw.head())\n",
    "\n",
    "print(\"Tipos de datos (train):\")\n",
    "display(train_raw.dtypes)\n",
    "\n",
    "print(\"Describe numérico (train):\")\n",
    "display(train_raw.describe(numeric_only=True))\n",
    "\n",
    "print(\"Describe del target (train):\")\n",
    "display(train_raw[TARGET].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3d5e7",
   "metadata": {},
   "source": [
    "## 3. Calidad de datos (completitud, unicidad, consistencia, validez)\n",
    "\n",
    "Esta sección replica el esquema de los notebooks:\n",
    "- Completitud: porcentaje de faltantes por columna.\n",
    "- Unicidad: duplicados exactos.\n",
    "- Consistencia: categorías con formatos incoherentes (mayúsculas, espacios, etc).\n",
    "- Validez: valores imposibles o fuera de rango lógico (por ejemplo LDL negativo).\n",
    "\n",
    "La salida de esta sección es la base para justificar tus decisiones de limpieza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a319a1e",
   "metadata": {},
   "source": [
    "### 3.1 Completitud (missing values)\n",
    "\n",
    "Qué haces: calculas el porcentaje de faltantes por columna.\n",
    "\n",
    "Por qué: si tienes faltantes, debes decidir si imputas, corriges usando otras columnas (por ejemplo BP), o eliminas filas (por ejemplo si falta el target).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fa356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_train = (train_raw.isna().mean() * 100).sort_values(ascending=False)\n",
    "missing_test  = (test_raw.isna().mean() * 100).sort_values(ascending=False)\n",
    "\n",
    "print(\"Missing % (train) - top 15\")\n",
    "display(missing_train.head(15))\n",
    "\n",
    "print(\"Missing % (test) - top 15\")\n",
    "display(missing_test.head(15))\n",
    "\n",
    "print(\"Filas en train con target missing:\", train_raw[TARGET].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb8169",
   "metadata": {},
   "source": [
    "### 3.2 Unicidad (duplicados)\n",
    "\n",
    "Qué haces: cuentas duplicados exactos (filas idénticas).\n",
    "\n",
    "Por qué: duplicados exactos pueden sesgar el entrenamiento porque repites el mismo ejemplo varias veces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3626dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dup_train = train_raw.duplicated().sum()\n",
    "dup_test  = test_raw.duplicated().sum()\n",
    "\n",
    "print(\"Duplicados exactos train:\", dup_train)\n",
    "print(\"Duplicados exactos test :\", dup_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3ec2c",
   "metadata": {},
   "source": [
    "### 3.3 Consistencia (categóricas)\n",
    "\n",
    "Qué haces: revisas `value_counts()` de columnas categóricas.\n",
    "\n",
    "Por qué: detecta espacios al inicio/final, mezcla de mayúsculas/minúsculas o etiquetas duplicadas con distinto formato. La regla general es normalizar (strip y formato consistente).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d23712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_cols = [\n",
    "    \"Sex\",\n",
    "    \"Smoking Status\",\n",
    "    \"Diabetes Status\",\n",
    "    \"Physical Activity Level\",\n",
    "    \"Family History of CVD\",\n",
    "    \"Blood Pressure Category\",\n",
    "]\n",
    "\n",
    "for c in cat_cols:\n",
    "    print(\"\\n---\", c, \"---\")\n",
    "    display(train_raw[c].astype(str).value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5053d6e",
   "metadata": {},
   "source": [
    "### 3.4 Validez (rangos y valores imposibles)\n",
    "\n",
    "Qué haces: detectas valores imposibles o fuera de rango lógico.\n",
    "\n",
    "Por qué: un valor inválido no se debe imputar tal cual. Primero se corrige a `NaN` o se transforma.\n",
    "Ejemplos claros en estos datos: LDL negativo y colesterol total negativo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "invalid_ldl = (train_raw[\"Estimated LDL (mg/dL)\"] < 0).sum()\n",
    "invalid_chol = (train_raw[\"Total Cholesterol (mg/dL)\"] < 0).sum()\n",
    "\n",
    "print(\"LDL negativo (train):\", invalid_ldl)\n",
    "print(\"Colesterol total negativo (train):\", invalid_chol)\n",
    "\n",
    "print(\"Peso <= 0 (train):\", (train_raw[\"Weight (kg)\"] <= 0).sum(skipna=True))\n",
    "print(\"Altura (m) <= 0 (train):\", (train_raw[\"Height (m)\"] <= 0).sum(skipna=True))\n",
    "print(\"Edad <= 0 (train):\", (train_raw[\"Age\"] <= 0).sum(skipna=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65dac6",
   "metadata": {},
   "source": [
    "## 4. Limpieza y preparación (decisiones justificadas)\n",
    "\n",
    "Aquí aplicas decisiones basadas en la sección anterior:\n",
    "1. Eliminar duplicados exactos en train.\n",
    "2. Eliminar filas sin target.\n",
    "3. Validez: convertir LDL negativo y colesterol total negativo a `NaN`.\n",
    "4. Usar `Blood Pressure (mmHg)` para completar `Systolic BP` y `Diastolic BP` cuando falten.\n",
    "5. Normalizar categóricas: `strip()` para evitar espacios invisibles.\n",
    "\n",
    "Esto sigue el enfoque de `LimpiezaTransformacion.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb59637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_bp_series(series: pd.Series):\n",
    "    # Convierte strings tipo '112/83' en dos series numéricas: sistólica y diastólica.\n",
    "    sys_vals, dia_vals = [], []\n",
    "    for v in series.astype(str):\n",
    "        m = re.match(r\"^\\s*(\\d+)\\s*/\\s*(\\d+)\\s*$\", v)\n",
    "        if m:\n",
    "            sys_vals.append(float(m.group(1)))\n",
    "            dia_vals.append(float(m.group(2)))\n",
    "        else:\n",
    "            sys_vals.append(np.nan)\n",
    "            dia_vals.append(np.nan)\n",
    "    return pd.Series(sys_vals, index=series.index), pd.Series(dia_vals, index=series.index)\n",
    "\n",
    "def normalize_categories(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype(\"string\").str.strip()\n",
    "    return df\n",
    "\n",
    "def clean_common(df: pd.DataFrame, cat_cols: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Consistencia: quitar espacios en categóricas\n",
    "    df = normalize_categories(df, cat_cols)\n",
    "\n",
    "    # Validez: valores imposibles a NaN\n",
    "    df.loc[df[\"Estimated LDL (mg/dL)\"] < 0, \"Estimated LDL (mg/dL)\"] = np.nan\n",
    "    df.loc[df[\"Total Cholesterol (mg/dL)\"] < 0, \"Total Cholesterol (mg/dL)\"] = np.nan\n",
    "\n",
    "    # Completar BP desde el string si es posible\n",
    "    if \"Blood Pressure (mmHg)\" in df.columns:\n",
    "        sys_bp, dia_bp = parse_bp_series(df[\"Blood Pressure (mmHg)\"])\n",
    "        df[\"Systolic BP\"] = df[\"Systolic BP\"].fillna(sys_bp)\n",
    "        df[\"Diastolic BP\"] = df[\"Diastolic BP\"].fillna(dia_bp)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Aplicar reglas de limpieza a train y test\n",
    "train = train_raw.drop_duplicates().copy()\n",
    "train = train.dropna(subset=[TARGET]).copy()\n",
    "train = clean_common(train, cat_cols)\n",
    "\n",
    "test = test_raw.drop_duplicates().copy()\n",
    "test = clean_common(test, cat_cols)\n",
    "\n",
    "print(\"Train limpio shape:\", train.shape)\n",
    "print(\"Test  limpio shape:\", test.shape)\n",
    "\n",
    "# Verificación: ahora LDL y colesterol negativos deberían ser 0\n",
    "print(\"LDL negativo (train limpio):\", (train[\"Estimated LDL (mg/dL)\"] < 0).sum())\n",
    "print(\"Colesterol total negativo (train limpio):\", (train[\"Total Cholesterol (mg/dL)\"] < 0).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e6727",
   "metadata": {},
   "source": [
    "## 5. Exploración útil para modelado (visualizaciones)\n",
    "\n",
    "Siguiendo `ManipulacionExploracionCalidad.ipynb`:\n",
    "- Histograma del target\n",
    "- Matriz de correlación numérica\n",
    "- Top correlaciones con el target\n",
    "\n",
    "Por qué: permite justificar ingeniería de características y entender qué variables parecen más relacionadas con el riesgo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(train[TARGET], kde=True)\n",
    "plt.title(\"Distribución de CVD Risk Score\")\n",
    "plt.xlabel(TARGET)\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "corr = train.select_dtypes(include=[np.number]).corr()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Heatmap de correlación (numéricas)\")\n",
    "plt.show()\n",
    "\n",
    "target_corr = corr[TARGET].drop(TARGET).abs().sort_values(ascending=False)\n",
    "display(target_corr.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f473c",
   "metadata": {},
   "source": [
    "## 6. Definir X, y y partición train-test (obligatorio del enunciado)\n",
    "\n",
    "Qué haces:\n",
    "- Separas X e y.\n",
    "- Haces `train_test_split` con `random_state=42` y `test_size=0.25`.\n",
    "\n",
    "Por qué: es una instrucción explícita del enunciado y crea el conjunto test para comparar modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DROP_ALWAYS = [\"Patient ID\", \"Date of Service\", AUX_LABEL]\n",
    "\n",
    "X = train.drop(columns=[TARGET] + DROP_ALWAYS).copy()\n",
    "y = train[TARGET].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e648e6",
   "metadata": {},
   "source": [
    "## 7. Modelos: dos estrategias de preparación (con pipelines)\n",
    "\n",
    "El enunciado pide al menos 2 modelos con preparación distinta.\n",
    "\n",
    "Modelo 1 (baseline)\n",
    "- Imputación + OHE\n",
    "- Sin escalado numérico\n",
    "- Sin ingeniería adicional\n",
    "\n",
    "Modelo 2 (mejorado)\n",
    "- Ingeniería de características simple (lineal)\n",
    "  - Pulse Pressure = Systolic BP - Diastolic BP\n",
    "  - Chol/HDL = Total Cholesterol / HDL\n",
    "  - LDL/HDL = Estimated LDL / HDL\n",
    "- Escalado numérico (StandardScaler)\n",
    "\n",
    "Ambos modelos usan `Pipeline` como en `RegresionLineal.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    # Transformer scikit-learn que recibe DataFrame y devuelve DataFrame con columnas extra.\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        if \"Systolic BP\" in X.columns and \"Diastolic BP\" in X.columns:\n",
    "            X[\"Pulse Pressure\"] = X[\"Systolic BP\"] - X[\"Diastolic BP\"]\n",
    "\n",
    "        if \"Total Cholesterol (mg/dL)\" in X.columns and \"HDL (mg/dL)\" in X.columns:\n",
    "            X[\"Chol/HDL\"] = X[\"Total Cholesterol (mg/dL)\"] / X[\"HDL (mg/dL)\"]\n",
    "\n",
    "        if \"Estimated LDL (mg/dL)\" in X.columns and \"HDL (mg/dL)\" in X.columns:\n",
    "            X[\"LDL/HDL\"] = X[\"Estimated LDL (mg/dL)\"] / X[\"HDL (mg/dL)\"]\n",
    "\n",
    "        return X\n",
    "\n",
    "def build_preprocess(X_sample: pd.DataFrame, scale_numeric: bool) -> ColumnTransformer:\n",
    "    num_cols = X_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols_local = [c for c in X_sample.columns if c not in num_cols]\n",
    "\n",
    "    num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "    if scale_numeric:\n",
    "        num_steps.append((\"scaler\", StandardScaler()))\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline(num_steps), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\")),\n",
    "            ]), cat_cols_local),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return preprocess\n",
    "\n",
    "# Modelo 1\n",
    "preprocess_1 = build_preprocess(X_train, scale_numeric=False)\n",
    "model_1 = Pipeline([\n",
    "    (\"preprocess\", preprocess_1),\n",
    "    (\"reg\", LinearRegression()),\n",
    "])\n",
    "\n",
    "# Modelo 2\n",
    "X_train_fe = FeatureEngineer().transform(X_train)\n",
    "preprocess_2 = build_preprocess(X_train_fe, scale_numeric=True)\n",
    "model_2 = Pipeline([\n",
    "    (\"fe\", FeatureEngineer()),\n",
    "    (\"preprocess\", preprocess_2),\n",
    "    (\"reg\", LinearRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a259a6d",
   "metadata": {},
   "source": [
    "## 8. Entrenar y evaluar los 2 modelos (RMSE, MAE, R2)\n",
    "\n",
    "Qué haces:\n",
    "- Entrenas en `X_train, y_train`.\n",
    "- Predices en `X_test`.\n",
    "- Calculas RMSE, MAE, R2 y construyes una tabla comparativa.\n",
    "\n",
    "Por qué: corresponde a la evaluación cuantitativa del enunciado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29893c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, X_tr, y_tr, X_te, y_te):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_te)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, pred))\n",
    "    mae  = mean_absolute_error(y_te, pred)\n",
    "    r2   = r2_score(y_te, pred)\n",
    "\n",
    "    return rmse, mae, r2\n",
    "\n",
    "rmse1, mae1, r21 = evaluate(model_1, X_train, y_train, X_test, y_test)\n",
    "rmse2, mae2, r22 = evaluate(model_2, X_train, y_train, X_test, y_test)\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    {\"Modelo\": \"Modelo 1 (baseline)\", \"RMSE\": rmse1, \"MAE\": mae1, \"R2\": r21},\n",
    "    {\"Modelo\": \"Modelo 2 (feat eng + scaler)\", \"RMSE\": rmse2, \"MAE\": mae2, \"R2\": r22},\n",
    "]).sort_values(\"RMSE\", ascending=True)\n",
    "\n",
    "display(results)\n",
    "\n",
    "best_model_name = results.iloc[0][\"Modelo\"]\n",
    "best_model = model_1 if best_model_name.startswith(\"Modelo 1\") else model_2\n",
    "print(\"Mejor modelo por RMSE:\", best_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b59524",
   "metadata": {},
   "source": [
    "## 9. Interpretación del mejor modelo: coeficientes e importancia de variables\n",
    "\n",
    "Qué haces:\n",
    "- Extraes nombres de variables post preprocesamiento.\n",
    "- Extraes coeficientes del regresor.\n",
    "- Ordenas por valor absoluto.\n",
    "\n",
    "Por qué: el enunciado exige coeficientes e importancia de variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab08c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "preprocess_step = best_model.named_steps[\"preprocess\"]\n",
    "feature_names = preprocess_step.get_feature_names_out()\n",
    "coefs = best_model.named_steps[\"reg\"].coef_\n",
    "\n",
    "coef_df = (\n",
    "    pd.DataFrame({\"Feature\": feature_names, \"Coef\": coefs})\n",
    "      .assign(AbsCoef=lambda d: d[\"Coef\"].abs())\n",
    "      .sort_values(\"AbsCoef\", ascending=False)\n",
    ")\n",
    "\n",
    "display(coef_df.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e4437",
   "metadata": {},
   "source": [
    "## 10. Verificación de supuestos (gráficos)\n",
    "\n",
    "Qué haces:\n",
    "- Residuos vs predicción: homocedasticidad y linealidad (visual).\n",
    "- Histograma y QQ-plot: normalidad aproximada de errores.\n",
    "\n",
    "Por qué: el enunciado pide validar supuestos para interpretación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e402511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.4)\n",
    "plt.axhline(0)\n",
    "plt.title(\"Residuos vs Predicción\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Residuo (y - y_hat)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Histograma de residuos\")\n",
    "plt.xlabel(\"Residuo\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n",
    "\n",
    "import scipy.stats as stats\n",
    "plt.figure(figsize=(6,4))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ-plot de residuos\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499bc2",
   "metadata": {},
   "source": [
    "### 10.1 Pruebas estadísticas opcionales (statsmodels)\n",
    "\n",
    "Si quieres replicar el rigor de `RegresionLineal.ipynb`, instala statsmodels:\n",
    "`pip install statsmodels`\n",
    "\n",
    "Esta celda:\n",
    "- Durbin-Watson: independencia aproximada\n",
    "- Breusch-Pagan: homocedasticidad\n",
    "- VIF: multicolinealidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "    Xt_test = preprocess_step.transform(X_test)\n",
    "    Xt_test_arr = Xt_test.toarray() if hasattr(Xt_test, \"toarray\") else Xt_test\n",
    "    Xt_test_df = pd.DataFrame(Xt_test_arr, columns=feature_names, index=X_test.index)\n",
    "\n",
    "    X_sm = sm.add_constant(Xt_test_df)\n",
    "    model_sm = sm.OLS(y_test, X_sm).fit()\n",
    "\n",
    "    dw = durbin_watson(model_sm.resid)\n",
    "    print(\"Durbin-Watson:\", dw)\n",
    "\n",
    "    bp_test = het_breuschpagan(model_sm.resid, model_sm.model.exog)\n",
    "    bp_labels = [\"LM Stat\", \"LM p-value\", \"F Stat\", \"F p-value\"]\n",
    "    print(\"Breusch-Pagan:\", dict(zip(bp_labels, bp_test)))\n",
    "\n",
    "    vif_vals = []\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        for i in range(Xt_test_df.shape[1]):\n",
    "            vif_vals.append(variance_inflation_factor(Xt_test_df.values, i))\n",
    "\n",
    "    vif_df = pd.DataFrame({\"Feature\": feature_names, \"VIF\": vif_vals}).sort_values(\"VIF\", ascending=False)\n",
    "    display(vif_df.head(25))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"No se pudo importar statsmodels. Si quieres estas pruebas: pip install statsmodels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c578e6",
   "metadata": {},
   "source": [
    "## 11. Entrenar el mejor modelo con TODO el train y predecir el test no etiquetado\n",
    "\n",
    "Qué haces:\n",
    "- Reentrenas con todo el train.\n",
    "- Predices `CVD Risk Score` para el test.\n",
    "- Exportas un CSV listo para entregar.\n",
    "\n",
    "Por qué: es un entregable explícito del enunciado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_full = train.drop(columns=[TARGET] + DROP_ALWAYS).copy()\n",
    "y_full = train[TARGET].copy()\n",
    "\n",
    "X_submit = test.drop(columns=DROP_ALWAYS).copy()\n",
    "\n",
    "best_model.fit(X_full, y_full)\n",
    "test_pred = best_model.predict(X_submit)\n",
    "\n",
    "out = test_raw.copy()\n",
    "out[TARGET] = test_pred\n",
    "\n",
    "output_path = \"Datos Test Lab 1.csv\"\n",
    "out.to_csv(output_path, index=False, sep=\";\")\n",
    "\n",
    "print(\"Archivo generado para entrega:\", output_path)\n",
    "display(out.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1266b46",
   "metadata": {},
   "source": [
    "## 12. Preguntas de análisis de resultados (responde aquí)\n",
    "\n",
    "Responde directamente las preguntas del enunciado usando tus resultados:\n",
    "1. Coeficientes del mejor modelo.\n",
    "2. Mejor rendimiento y cómo interpretar RMSE, MAE y R2.\n",
    "3. Variables seleccionadas e interpretación en el contexto.\n",
    "4. Forma matemática de la regresión lineal y método (mínimos cuadrados ordinarios).\n",
    "5. Dos tipos de sesgo que podrían afectar el resultado.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
